obs
test.dif
test.dif = data.frame(tail(data.dif,4))
test.dif
#for NQ - differenced
preds <- preds.allD$fcst$NQ[,'fcst']
obs <- test.dif$NQ
#MAPE
mean(abs(preds - obs)/abs(obs)) #.01621
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #.336
test
test.dif
predDow$pred
#for SP
preds <- preds.all$fcst$SP[,'fcst']
obs <- test$SP
#MAPE
mean(abs(preds - obs)/abs(obs)) #.01621
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #.336
#for SP - differenced
preds <- preds.allD$fcst$SP[,'fcst']
obs <- test.dif$SP
#MAPE
mean(abs(preds - obs)/abs(obs)) #140.004
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #2204.096
#for Dow
preds <- preds.all$fcst$Dow[,'fcst']
obs <- test$Dow
#MAPE
mean(abs(preds - obs)/abs(obs)) #.018
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #1.17
#for Dow - differenced
preds <- preds.allD$fcst$Dow[,'fcst']
obs <- test.dif$Dow
#MAPE
mean(abs(preds - obs)/abs(obs)) #353.69
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #7749.012
preds.allD <- as.vector(predict(modD,n.ahead=4))
#for NQ - differenced
preds <- preds.allD$fcst$NQ[,'fcst']
obs <- test.dif$NQ
#MAPE
mean(abs(preds - obs)/abs(obs)) #140.004
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #2204.096
#for SP - differenced
preds <- preds.allD$fcst$SP[,'fcst']
obs <- test.dif$SP
#MAPE
mean(abs(preds - obs)/abs(obs)) #353.69
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #7749.012
preds <- preds.allD$fcst$Dow[,'fcst']
obs <- test.dif$Dow
#MAPE
mean(abs(preds - obs)/abs(obs)) #63.8
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #4513
train.dif[,'Dow']
test_modelAGG <- function(m,n){
spec = ugarchspec(variance.model=list(garchOrder=c(m,n)),
mean.model=list(armaOrder=c(3,3),
include.mean=T), distribution.model="std")
fit = ugarchfit(spec, train.dif[,'Dow'], solver = 'hybrid')
current.bic = infocriteria(fit)[2]
df = data.frame(m,n,current.bic)
names(df) <- c("m","n","BIC")
print(paste(m,n,current.bic,sep=" "))
return(df)
}
orders = data.frame(Inf,Inf,Inf)
names(orders) <- c("m","n","BIC")
for (m in 0:2){
for (n in 0:2){
possibleError <- tryCatch(
orders<-rbind(orders,test_modelAGG(m,n)),
error=function(e) e
)
if(inherits(possibleError, "error")) next
}
}
orders <- orders[order(-orders$BIC),]
tail(orders)
# #ARMA update
# #ARIMA-GARCH ARIMA order
test_modelAGA <- function(p,q){
spec = ugarchspec(variance.model=list(garchOrder=c(1,1)),
mean.model=list(armaOrder=c(p,q),
include.mean=T), distribution.model="std")
fit = ugarchfit(spec, train.dif[,'Dow'], solver = 'hybrid')
current.bic = infocriteria(fit)[2]
df = data.frame(p,q,current.bic)
names(df) <- c("p","q","BIC")
print(paste(p,q,current.bic,sep=" "))
return(df)
}
orders = data.frame(Inf,Inf,Inf)
names(orders) <- c("p","q","BIC")
for (p in 0:4){
for (q in 0:4){
possibleError <- tryCatch(
orders<-rbind(orders,test_modelAGA(p,q)),
error=function(e) e
)
if(inherits(possibleError, "error")) next
}
}
orders <- orders[order(-orders$BIC),]
tail(orders)
test_modelAGG <- function(m,n){
spec = ugarchspec(variance.model=list(garchOrder=c(m,n)),
mean.model=list(armaOrder=c(2,1),
include.mean=T), distribution.model="std")
fit = ugarchfit(spec, train.dif[,'Dow'], solver = 'hybrid')
current.bic = infocriteria(fit)[2]
df = data.frame(m,n,current.bic)
names(df) <- c("m","n","BIC")
print(paste(m,n,current.bic,sep=" "))
return(df)
}
orders = data.frame(Inf,Inf,Inf)
names(orders) <- c("m","n","BIC")
for (m in 0:2){
for (n in 0:2){
possibleError <- tryCatch(
orders<-rbind(orders,test_modelAGG(m,n)),
error=function(e) e
)
if(inherits(possibleError, "error")) next
}
}
orders <- orders[order(-orders$BIC),]
tail(orders)
#1,1
final.model = garchFit(~ arma(2,1)+ garch(1,1), data=train.dif[,'Dow'], trace = FALSE)
summary(final.model)
train.gar = train.dif[,'Dow']
test = train.gar[(length(train.gar)-47):length(train.gar)]
train = train.gar[1:(length(train.gar)-48)]
nfore = length(test)
fore.series = NULL
spec = ugarchspec(variance.model=list(garchOrder=c(1,1)),
mean.model=list(armaOrder=c(2, 1),
include.mean=T), distribution.model="std")
for(f in 1: nfore){
## Fit models
data = train
if(f>=2)
data = c(train,test[1:(f-1)])
final.model = ugarchfit(spec, data, solver = 'hybrid')
## Forecast
fore = ugarchforecast(final.model, n.ahead=1)
fore.series = c(fore.series, fore@forecast$seriesFor)
}
#Accuracy measures
preds <- fore.series
obs <- tail(train.gar,48)
#MAPE
mean(abs(preds - obs)/abs(obs)) #.057
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #.308
data <- read.csv(fname)
data <- data[,c(2,3,4)]
data.dif <- cbind(diff(data$NQ),diff(data$SP),diff(data$Dow))
colnames(data.dif) = c('NQ','SP','Dow')
train = data[0:(nrow(data)-4),]
test = tail(data,4)
train.dif = data.frame(data.dif[0:(nrow(data.dif)-4),])
test.dif = data.frame(tail(data.dif,4))
par(mfrow=c(1,3))
plot(train$NQ, type ="l")#Upward trend, fairly constant variance, no clear seasonality.
plot(train$SP, type ="l")#upward trend, less constant variance, no clear seasonality.
plot(train$Dow, type ="l")#upward trend, less constant variance, no clear seasonality.
acf(train$NQ, lag.max = 40) #For Sure trend, and nonconstant autocovariance.
acf(train$SP, lag.max = 40)#For Sure trend, and nonconstant autocovariance.
acf(train$Dow, lag.max = 40)#For Sure trend, and nonconstant autocovariance.
plot(train.dif[,"NQ"], type ="l")#Hard to see upward trend, variance seems to fluctuate, no clear seasonality.
plot(train.dif[,"SP"], type ="l")#Unclear trend, less constant variance, no clear seasonality.
plot(train.dif[,"Dow"], type ="l")#Unclear trend, less constant variance, no clear seasonality.
acf(train.dif[,"NQ"], lag.max = 40) #Looks mostly like white noise except for lag 4 and 31, which are outside the bands
acf(train.dif[,"SP"], lag.max = 40)#Mostly like white noise except for a couple bands
acf(train.dif[,"Dow"], lag.max = 40)##Mostly like white noise except for a couple bands
time.pts = c(1:nrow(train))
time.pts = c(time.pts - min(time.pts))/max(time.pts)
x1 = time.pts
x2 = time.pts^2
x3 = time.pts^3
lm.fit.NQ = lm(train$NQ~x1+x2+x3)
lm.fit.SP = lm(train$SP~x1+x2+x3)
lm.fit.Dow = lm(train$Dow~x1+x2+x3)
resNQ = residuals(lm.fit.NQ)
resSP = residuals(lm.fit.SP)
resDow = residuals(lm.fit.Dow)
time.pts = c(1:nrow(train))
time.pts = c(time.pts - min(time.pts))/max(time.pts)
x1 = time.pts
x2 = time.pts^2
x3 = time.pts^3
lm.fit.NQ = lm(train$NQ~x1+x2+x3)
lm.fit.SP = lm(train$SP~x1+x2+x3)
lm.fit.Dow = lm(train$Dow~x1+x2+x3)
resNQ = residuals(lm.fit.NQ)
resSP = residuals(lm.fit.SP)
resDow = residuals(lm.fit.Dow)
plot(resNQ, type = "l")#Mean looks about constant now, variance looks mostly constant, no seasonality.
acf(resNQ, lag.max = 40)#Seems to tail off after 10 or so lags
pacf(resNQ, lag.max = 40)#Cuts off after 1
#for NQ, there still seems to be patterns of siginificant correlation and therefore not stationary.
plot(resSP, type = "l")
acf(resSP, lag.max = 40)
pacf(resSP, lag.max = 40)
#for SP, the ACF trailts off and the PACF cuts off after 1 lag (for the most part), this might be stationary, but
#the residual plots still have what seems like a bit of a trend and a non-constant variance so probably still not
#stationary.
plot(resDow, type = "l")
acf(resDow, lag.max = 40)
pacf(resDow, lag.max = 40)
#for Dow, the ACF trailts off and the PACF cuts off after 1 lag (for the most part), this might be stationary, but
#the residual plots still have what seems like a bit of a trend and a non-constant variance so probably still not
#stationary.
#For NQ
preds <- lm.fit.NQ$fitted.values
obs <- train$NQ
#MAPE
mean(abs(preds - obs)/abs(obs)) #.05
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #.065
#For SP
preds <- lm.fit.SP$fitted.values
obs <- train$SP
#MAPE
mean(abs(preds - obs)/abs(obs)) #.03
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #.102
#For Dow
preds <- lm.fit.Dow$fitted.values
obs <- train$Dow
#MAPE
mean(abs(preds - obs)/abs(obs)) #.034
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #.089
#4 For the trend, yes this simple model seems to capture the overall shapes and direction rather well.
plot(lm.fit.NQ$fitted.values, type ="l")#Upward trend, fairly constant variance, no clear seasonality.
plot(lm.fit.SP$fitted.values, type ="l")#upward trend, less constant variance, no clear seasonality.
plot(lm.fit.Dow$fitted.values, type ="l")#upward trend, less constant variance, no clear seasonality.
plot(train$NQ, type ="l")#Upward trend, fairly constant variance, no clear seasonality.
plot(train$SP, type ="l")#upward trend, less constant variance, no clear seasonality.
plot(train$Dow, type ="l")#upward trend, less constant variance, no clear seasonality.
par(mfrow=c(1,2))
acf(data$NQ, lag.max = 40)
pacf(data$NQ, lag.max = 40)
#Even though the PACF plot cuts off at 1, becuase the ACF plot seems to have high ACF for nearly ever. It's hard
#to determine a good order. Though if it had to be any, it would have (p,d,1)
acf(data$SP, lag.max = 40)
pacf(data$SP, lag.max = 40)
#Even though the PACF plot cuts off at 1, becuase the ACF plot seems to have high ACF for nearly ever. It's hard
#to determine a good order. Though if it had to be any, it would have (p,d,1)
acf(data$Dow, lag.max = 40)
pacf(data$Dow, lag.max = 40)
train$NQ
#ARIMA order selection, NQ
test_modelA <- function(p,d,q){
mod = arima(train.dif$NQ, order=c(p,d,q), method="ML")
current.aic = AIC(mod)
df = data.frame(p,d,q,current.aic)
names(df) <- c("p","d","q","AIC")
print(paste(p,d,q,current.aic,sep=" "))
return(df)
}
orders = data.frame(Inf,Inf,Inf,Inf)
names(orders) <- c("p","d","q","AIC")
for (p in 0:4){
for (d in 0:2){
for (q in 0:4) {
possibleError <- tryCatch(
orders<-rbind(orders,test_modelA(p,d,q)),
error=function(e) e
)
if(inherits(possibleError, "error")) next
}
}
}
orders <- orders[order(-orders$AIC),]
tail(orders)
arimaNQ = arima(train.dif$NQ, order=c(2,1,3), method="ML")
#ARIMA order selection, SP
test_modelA <- function(p,d,q){
mod = arima(train.dif$SP, order=c(p,d,q), method="ML")
current.aic = AIC(mod)
df = data.frame(p,d,q,current.aic)
names(df) <- c("p","d","q","AIC")
print(paste(p,d,q,current.aic,sep=" "))
return(df)
}
orders = data.frame(Inf,Inf,Inf,Inf)
names(orders) <- c("p","d","q","AIC")
for (p in 0:4){
for (d in 0:2){
for (q in 0:4) {
possibleError <- tryCatch(
orders<-rbind(orders,test_modelA(p,d,q)),
error=function(e) e
)
if(inherits(possibleError, "error")) next
}
}
}
orders <- orders[order(-orders$AIC),]
tail(orders)
arimaSP = arima(train.dif$SP, order=c(1,0,4), method="ML")
#ARIMA order selection, Dow
test_modelA <- function(p,d,q){
mod = arima(train.dif$Dow, order=c(p,d,q), method="ML")
current.aic = AIC(mod)
df = data.frame(p,d,q,current.aic)
names(df) <- c("p","d","q","AIC")
print(paste(p,d,q,current.aic,sep=" "))
return(df)
}
orders = data.frame(Inf,Inf,Inf,Inf)
names(orders) <- c("p","d","q","AIC")
for (p in 0:4){
for (d in 0:2){
for (q in 0:4) {
possibleError <- tryCatch(
orders<-rbind(orders,test_modelA(p,d,q)),
error=function(e) e
)
if(inherits(possibleError, "error")) next
}
}
}
orders <- orders[order(-orders$AIC),]
tail(orders)
#For NQ
#AR Roots
round(abs( polyroot( c(1 , coef(arimaNQ)[1:2]) )),3)
#.662, 1.712
#MA Roots
round(abs( polyroot( c(1 , coef(arimaNQ)[(3):(5)]) )),3)
# 1.018, 1.007, 1.018
#For NQ: Process is Stationary, not Causal, and Invertible
#For SP
#AR Roots
round(abs( polyroot( c(1 , coef(arimaSP)[1:1]) )),3)
#1.218
#MA Roots
round(abs( polyroot( c(1 , coef(arimaSP)[(2):(5)]) )),3)
# 1.000, 1.802, 1.629, 1.629
#For SP: Process is Stationary, Causal, and not Invertible
#For Dow
#AR Roots
round(abs( polyroot( c(1 , coef(arimaDow)[1:2]) )),3)
#.580, 1.778
#MA Roots
round(abs( polyroot( c(1 , coef(arimaDow)[(3):(6)]) )),3)
# 1.000, 1.000, 1.000, 4.894
#For SP: Process is Stationary, Causal, and Invertible
predNQ = as.vector(predict(arimaNQ,n.ahead=4))
predSP = as.vector(predict(arimaSP,n.ahead=4))
predDow = as.vector(predict(arimaDow,n.ahead=4))
#for NQ
preds <- predNQ$pred
obs <- test.dif$NQ
#MAPE
mean(abs(preds - obs)/abs(obs)) #.0613
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #1.403
#for SP
preds <- predSP$pred
obs <- test.dif$SP
#MAPE
mean(abs(preds - obs)/abs(obs)) #2.29
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #3.37
#for Dow
preds <- predDow$pred
obs <- test.dif$Dow
#MAPE
mean(abs(preds - obs)/abs(obs)) #.072
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #1.67
Box.test(arimaNQ$residuals, type = "Box-Pierce", lag = 6, fitdf = 5)
#For NQ, this test is small if you're counting 5% confidence meaning We reject the null that this is uncorrelated.
Box.test(arimaSP$residuals, type = "Box-Pierce", lag = 8, fitdf = 7)
#For SP, this test is small if you're counting 5% confidence meaning We reject the null that this is uncorrelated.
Box.test(arimaDow$residuals, type = "Box-Pierce", lag = 7, fitdf = 6)
#For Dow, this test is very small meaning We reject the null that this is uncorrelated.
vs <- VARselect(train)
vsD <- VARselect(train.dif)
vs$selection
vsD$selection
mod <- VAR(train,p=8)
modD <- VAR(train.dif,p=7)
## ARCH, Residual Analysis: Constant Variance Assumption
arch.test(mod) #Reject Constant Variance
## J-B, Residual Analysis: Normality Assumption
normality.test(mod) #Reject Normality
## Portmantau, Residual Analysis: Uncorrelated Errors Assumption
serial.test(mod)#Accept Null Hypothesis that Residuals are uncorrelated.
## ARCH, Residual Analysis: Constant Variance Assumption
arch.test(modD) #Reject Constant Variance
## J-B, Residual Analysis: Normality Assumption
normality.test(modD) #Reject Normality
## Portmantau, Residual Analysis: Uncorrelated Errors Assumption
serial.test(modD)#Accept Null Hypothesis that Residuals are uncorrelated if your confidence is 1% otherwise reject
preds.all <- as.vector(predict(mod,n.ahead=4))
preds.allD <- as.vector(predict(modD,n.ahead=4))
#for NQ
preds <- preds.all$fcst$NQ[,'fcst']
obs <- test$NQ
#MAPE
mean(abs(preds - obs)/abs(obs)) #.01621
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #.336
#for NQ - differenced
preds <- preds.allD$fcst$NQ[,'fcst']
obs <- test.dif$NQ
#MAPE
mean(abs(preds - obs)/abs(obs)) #1.73
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #2.24
#for SP
preds <- preds.all$fcst$SP[,'fcst']
obs <- test$SP
#MAPE
mean(abs(preds - obs)/abs(obs)) #.018
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #1.17
#for SP - differenced
preds <- preds.allD$fcst$SP[,'fcst']
obs <- test.dif$SP
#MAPE
mean(abs(preds - obs)/abs(obs)) #6.2
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #6.82
#for Dow
preds <- preds.all$fcst$Dow[,'fcst']
obs <- test$Dow
#MAPE
mean(abs(preds - obs)/abs(obs)) #.024
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #3.35
#for Dow - differenced
preds <- preds.allD$fcst$Dow[,'fcst']
obs <- test.dif$Dow
#MAPE
mean(abs(preds - obs)/abs(obs)) #1.766
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #5.2
final.model = garchFit(~ arma(2,1)+ garch(1,1), data=train.dif[,'Dow'], trace = FALSE)
summary(final.model)
setwd("C:/Users/jonch/Desktop/Data Analytics Business/Week 6")
rm(list = ls())
if (!require(tidyverse)) install.packages("tidyverse")
library(tidyverse)
if (!require(psych)) install.packages("psych")
library(psych)
if (!require(Ecdat)) install.packages("Ecdat")
library(Ecdat)
mydata <- dplyr::filter(Ecdat::Star, classk=="small.class"|classk=="regular")
# only analyze small and regular size classes
str(mydata)
mydata <- mydata %>%
mutate(totalscore = tmathssk + treadssk) %>%
mutate(small = ifelse(classk=="small.class",1,0)) %>%
mutate(boy = ifelse(sex=="boy",1,0)) %>%
mutate(whiteother = ifelse(race=="white"|race=="other",1,0)) %>%
mutate(freelunch = ifelse(freelunk=="yes",1,0)) %>%
mutate(schoolj = factor(schidkn))
str(mydata)
# get summary stats for small= 0 and small = 1
describeBy(mydata, mydata$small) # describeBy is from the psych package in R
# summary stats across all data
describe(mydata)
reg_0 <- lm(totalscore ~  1, data = dplyr::filter(mydata,small == 0))
summary(reg_0)
reg_1 <- lm(totalscore ~  1, data = dplyr::filter(mydata,small == 1))
summary(reg_1)
reg_all <- lm(totalscore ~  small, data = mydata)
summary(reg_all)
install.packages("PerformanceAnalytics")
library(PerformanceAnalytics)
library(xts)
library(lubridate)
setwd("C:/Users/jonch/Desktop/Data Analytics Business/Week 6")
#load data and create xts dataset
fund <- read.csv("contrafund.csv")
#converting dates to standard YYYY-MM-DD format
fund$Date <- mdy(fund$Date)
#Sorting data by dates
fund2<- fund[order(fund$Date),]
#create an xts dataset
All.dat <- xts(fund2[,-1],order.by = fund2[,1],)
#Calculate Compound Return for the fund across all the data
Return.cumulative(All.dat$ContraRet,geometric = TRUE)
#Cumulative Returns chart over time
#Check chart in Plots Tab on bottom right in R Studio
chart.CumReturns(All.dat$ContraRet,wealth.index = FALSE, geometric = TRUE)
############### Video 2: Measuring Risk
#Descriptive Statistics of the fund returns
#IMPORTANT NOTE - Arithmetic mean and standard deviation of returns are reported incorrectly
#The correct values as noticed when this code is run are:
#Arithmetic Mean Return = 1.17% and Standard Deviation = 4.32% per month
table.Stats(All.dat$ContraRet)
chart.CumReturns(All.dat$ContraRet,wealth.index = FALSE, geometric = TRUE)
fund2[,-1]
