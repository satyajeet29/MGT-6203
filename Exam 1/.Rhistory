#stationary.
plot(resDow, type = "l")
acf(resDow, lag.max = 40)
pacf(resDow, lag.max = 40)
#for Dow, the ACF trailts off and the PACF cuts off after 1 lag (for the most part), this might be stationary, but
#the residual plots still have what seems like a bit of a trend and a non-constant variance so probably still not
#stationary.
#For NQ
preds <- lm.fit.NQ$fitted.values
obs <- train$NQ
#MAPE
mean(abs(preds - obs)/abs(obs)) #.05
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #.065
#For SP
preds <- lm.fit.SP$fitted.values
obs <- train$SP
#MAPE
mean(abs(preds - obs)/abs(obs)) #.03
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #.102
#For Dow
preds <- lm.fit.Dow$fitted.values
obs <- train$Dow
#MAPE
mean(abs(preds - obs)/abs(obs)) #.034
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #.089
#4 For the trend, yes this simple model seems to capture the overall shapes and direction rather well.
plot(lm.fit.NQ$fitted.values, type ="l")#Upward trend, fairly constant variance, no clear seasonality.
plot(lm.fit.SP$fitted.values, type ="l")#upward trend, less constant variance, no clear seasonality.
plot(lm.fit.Dow$fitted.values, type ="l")#upward trend, less constant variance, no clear seasonality.
plot(train$NQ, type ="l")#Upward trend, fairly constant variance, no clear seasonality.
plot(train$SP, type ="l")#upward trend, less constant variance, no clear seasonality.
plot(train$Dow, type ="l")#upward trend, less constant variance, no clear seasonality.
par(mfrow=c(1,2))
acf(data$NQ, lag.max = 40)
pacf(data$NQ, lag.max = 40)
#Even though the PACF plot cuts off at 1, becuase the ACF plot seems to have high ACF for nearly ever. It's hard
#to determine a good order. Though if it had to be any, it would have (p,d,1)
acf(data$SP, lag.max = 40)
pacf(data$SP, lag.max = 40)
#Even though the PACF plot cuts off at 1, becuase the ACF plot seems to have high ACF for nearly ever. It's hard
#to determine a good order. Though if it had to be any, it would have (p,d,1)
acf(data$Dow, lag.max = 40)
pacf(data$Dow, lag.max = 40)
train$NQ
#ARIMA order selection, NQ
test_modelA <- function(p,d,q){
mod = arima(train.dif$NQ, order=c(p,d,q), method="ML")
current.aic = AIC(mod)
df = data.frame(p,d,q,current.aic)
names(df) <- c("p","d","q","AIC")
print(paste(p,d,q,current.aic,sep=" "))
return(df)
}
orders = data.frame(Inf,Inf,Inf,Inf)
names(orders) <- c("p","d","q","AIC")
for (p in 0:4){
for (d in 0:2){
for (q in 0:4) {
possibleError <- tryCatch(
orders<-rbind(orders,test_modelA(p,d,q)),
error=function(e) e
)
if(inherits(possibleError, "error")) next
}
}
}
orders <- orders[order(-orders$AIC),]
tail(orders)
arimaNQ = arima(train.dif$NQ, order=c(2,1,3), method="ML")
#ARIMA order selection, SP
test_modelA <- function(p,d,q){
mod = arima(train.dif$SP, order=c(p,d,q), method="ML")
current.aic = AIC(mod)
df = data.frame(p,d,q,current.aic)
names(df) <- c("p","d","q","AIC")
print(paste(p,d,q,current.aic,sep=" "))
return(df)
}
orders = data.frame(Inf,Inf,Inf,Inf)
names(orders) <- c("p","d","q","AIC")
for (p in 0:4){
for (d in 0:2){
for (q in 0:4) {
possibleError <- tryCatch(
orders<-rbind(orders,test_modelA(p,d,q)),
error=function(e) e
)
if(inherits(possibleError, "error")) next
}
}
}
orders <- orders[order(-orders$AIC),]
tail(orders)
arimaSP = arima(train.dif$SP, order=c(1,0,4), method="ML")
#ARIMA order selection, Dow
test_modelA <- function(p,d,q){
mod = arima(train.dif$Dow, order=c(p,d,q), method="ML")
current.aic = AIC(mod)
df = data.frame(p,d,q,current.aic)
names(df) <- c("p","d","q","AIC")
print(paste(p,d,q,current.aic,sep=" "))
return(df)
}
orders = data.frame(Inf,Inf,Inf,Inf)
names(orders) <- c("p","d","q","AIC")
for (p in 0:4){
for (d in 0:2){
for (q in 0:4) {
possibleError <- tryCatch(
orders<-rbind(orders,test_modelA(p,d,q)),
error=function(e) e
)
if(inherits(possibleError, "error")) next
}
}
}
orders <- orders[order(-orders$AIC),]
tail(orders)
#For NQ
#AR Roots
round(abs( polyroot( c(1 , coef(arimaNQ)[1:2]) )),3)
#.662, 1.712
#MA Roots
round(abs( polyroot( c(1 , coef(arimaNQ)[(3):(5)]) )),3)
# 1.018, 1.007, 1.018
#For NQ: Process is Stationary, not Causal, and Invertible
#For SP
#AR Roots
round(abs( polyroot( c(1 , coef(arimaSP)[1:1]) )),3)
#1.218
#MA Roots
round(abs( polyroot( c(1 , coef(arimaSP)[(2):(5)]) )),3)
# 1.000, 1.802, 1.629, 1.629
#For SP: Process is Stationary, Causal, and not Invertible
#For Dow
#AR Roots
round(abs( polyroot( c(1 , coef(arimaDow)[1:2]) )),3)
#.580, 1.778
#MA Roots
round(abs( polyroot( c(1 , coef(arimaDow)[(3):(6)]) )),3)
# 1.000, 1.000, 1.000, 4.894
#For SP: Process is Stationary, Causal, and Invertible
predNQ = as.vector(predict(arimaNQ,n.ahead=4))
predSP = as.vector(predict(arimaSP,n.ahead=4))
predDow = as.vector(predict(arimaDow,n.ahead=4))
#for NQ
preds <- predNQ$pred
obs <- test.dif$NQ
#MAPE
mean(abs(preds - obs)/abs(obs)) #.0613
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #1.403
#for SP
preds <- predSP$pred
obs <- test.dif$SP
#MAPE
mean(abs(preds - obs)/abs(obs)) #2.29
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #3.37
#for Dow
preds <- predDow$pred
obs <- test.dif$Dow
#MAPE
mean(abs(preds - obs)/abs(obs)) #.072
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #1.67
Box.test(arimaNQ$residuals, type = "Box-Pierce", lag = 6, fitdf = 5)
#For NQ, this test is small if you're counting 5% confidence meaning We reject the null that this is uncorrelated.
Box.test(arimaSP$residuals, type = "Box-Pierce", lag = 8, fitdf = 7)
#For SP, this test is small if you're counting 5% confidence meaning We reject the null that this is uncorrelated.
Box.test(arimaDow$residuals, type = "Box-Pierce", lag = 7, fitdf = 6)
#For Dow, this test is very small meaning We reject the null that this is uncorrelated.
vs <- VARselect(train)
vsD <- VARselect(train.dif)
vs$selection
vsD$selection
mod <- VAR(train,p=8)
modD <- VAR(train.dif,p=7)
## ARCH, Residual Analysis: Constant Variance Assumption
arch.test(mod) #Reject Constant Variance
## J-B, Residual Analysis: Normality Assumption
normality.test(mod) #Reject Normality
## Portmantau, Residual Analysis: Uncorrelated Errors Assumption
serial.test(mod)#Accept Null Hypothesis that Residuals are uncorrelated.
## ARCH, Residual Analysis: Constant Variance Assumption
arch.test(modD) #Reject Constant Variance
## J-B, Residual Analysis: Normality Assumption
normality.test(modD) #Reject Normality
## Portmantau, Residual Analysis: Uncorrelated Errors Assumption
serial.test(modD)#Accept Null Hypothesis that Residuals are uncorrelated if your confidence is 1% otherwise reject
preds.all <- as.vector(predict(mod,n.ahead=4))
preds.allD <- as.vector(predict(modD,n.ahead=4))
#for NQ
preds <- preds.all$fcst$NQ[,'fcst']
obs <- test$NQ
#MAPE
mean(abs(preds - obs)/abs(obs)) #.01621
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #.336
#for NQ - differenced
preds <- preds.allD$fcst$NQ[,'fcst']
obs <- test.dif$NQ
#MAPE
mean(abs(preds - obs)/abs(obs)) #1.73
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #2.24
#for SP
preds <- preds.all$fcst$SP[,'fcst']
obs <- test$SP
#MAPE
mean(abs(preds - obs)/abs(obs)) #.018
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #1.17
#for SP - differenced
preds <- preds.allD$fcst$SP[,'fcst']
obs <- test.dif$SP
#MAPE
mean(abs(preds - obs)/abs(obs)) #6.2
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #6.82
#for Dow
preds <- preds.all$fcst$Dow[,'fcst']
obs <- test$Dow
#MAPE
mean(abs(preds - obs)/abs(obs)) #.024
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #3.35
#for Dow - differenced
preds <- preds.allD$fcst$Dow[,'fcst']
obs <- test.dif$Dow
#MAPE
mean(abs(preds - obs)/abs(obs)) #1.766
#Precision
sum((preds-obs)^2)/sum((obs-mean(obs))^2) #5.2
final.model = garchFit(~ arma(2,1)+ garch(1,1), data=train.dif[,'Dow'], trace = FALSE)
summary(final.model)
struct()
list()
factor()
matrix()
list(2,4,1)
list(2,4,1)[-1]
list(2,4,1)[,-1]
list(2,4,1)[:-1]
list(2,4,1)[::-1]
list(2,4,1)[-1,]
list(2,4,1)
list(2,4,1)[:-1]
list(2,4,1)[-1,]
list(2,4,1)[-1:]
list(2,4,1)[-1::]
rm(list = ls())
library(ISLR)
data = College
View(data)
lm1 = lm(formula = data$Personal ~ data$Room.Board)
summary(lm1)
lnlog = lm(formula = data$Personal ~ log(data$Room.Board))
loglog = lm(formula = log(data$Personal ~ log(data$Room.Board)))
logln = lm(formula =  log(data$Personal ~ data$Room.Board))
summary(lnlog)
summary(loglog)
summary(logln)
View(data)
data$Personal
data$Room.Board
lnlog = lm(formula = data$Personal ~ log(data$Room.Board))
loglog = lm(formula = log(data$Personal) ~ log(data$Room.Board))
logln = lm(formula =  log(data$Personal) ~ data$Room.Board)
summary(lnlog)
summary(loglog)
summary(logln)
summary(loglog)
summary(lnlog)
summary(lm1)
summary(loglog) #.04489, .04366
setwd("C:/Users/jonch/Desktop/Data Analytics Business/Exam 1")
data = read.csv('binary.csv')
install.packages(c("ggExtra", "ROCR"))
m1 = lm(formula = data$Personal ~ data$Room.Board)
summary(lm1) #.039, .038
lnlog = lm(formula = data$Personal ~ log(data$Room.Board))
summary(lnlog) #.04037, .039
logln = lm(formula =  log(data$Personal) ~ data$Room.Board)
summary(logln) #.04304, .04181
logln = lm(formula =  log(data$Personal) ~ data$Room.Board)
loglog = lm(formula = log(data$Personal) ~ log(data$Room.Board))
data = College
library(ISLR)
data = College
lm1 = lm(formula = data$Personal ~ data$Room.Board)
summary(lm1) #.039, .038
lnlog = lm(formula = data$Personal ~ log(data$Room.Board))
summary(lnlog) #.04037, .0391
loglog = lm(formula = log(data$Personal) ~ log(data$Room.Board))
summary(loglog) #.04489, .04366
logln = lm(formula =  log(data$Personal) ~ data$Room.Board)
summary(logln) #.04304, .04181
lnlog = lm(formula = data$Personal ~ log(data$Room.Board))
summary(lnlog)
logln = lm(formula =  log(data$Personal) ~ data$Room.Board)
summary(logln) #.04304, .04181
loglog = lm(formula = log(data$Personal) ~ log(data$Room.Board))
summary(loglog) #.04489, .04366
data = read.csv('binary.csv')
bn = glm(formula = data$ï..admit ~ data$gre + data$gpa, family('binomial'))
summary(bn)
bn = glm(formula = data$ï..admit ~ data$gre + data$gpa, family("binomial"))
bn = glm(formula = data$ï..admit ~ data$gre + data$gpa, family = "binomial")
summary(bn)
predict(bn, type = c("response"))
library("ROCR")
class(prob)
library("ROCR")
prob = predict(bn, type = c("response"))
class(prob)
prob = prediction(bn, type = c("response"))
prob
data$ï..admit
problabls = ifelse(prob >= 0.5, 1, 0)
problabls
prediction(problabls, data$ï..admit)
pred = prediction(problabls, data$ï..admit)
class(pred)
performance(pred, measure = 'auc')
auc = performance(pred, measure = 'auc')
View(auc)
auc@y.values
prob = predict(bn, newdata = ., type = "response")
str(ISLR::Default)
df <- ISLR::Default
df<- df %>%
mutate(dft = ifelse(default=="Yes",1,0)) %>%
mutate(stdt = ifelse(student=="Yes",1,0))
if (!require(Ecdat)) install.packages("Ecdat")
library(Ecdat)
if (!require(ggplot2)) install.packages("ggplot2")
library(ggplot2)
if (!require(ISLR)) install.packages("ISLR")
library(ISLR)
if (!require(GGally)) install.packages("GGally")
library(GGally)
if (!require(car)) install.packages("car")
library(car)
if (!require(scatterplot3d)) install.packages("scatterplot3d")
library(scatterplot3d)
if (!require(tidyverse)) install.packages("tidyverse")
library(tidyverse)
if (!require(ggExtra))install.packages("ggExtra")
library("ggExtra")
if (!require(ROCR)) install.packages("ROCR")
library("ROCR")
str(ISLR::Default)
df <- ISLR::Default
df<- df %>%
mutate(dft = ifelse(default=="Yes",1,0)) %>%
mutate(stdt = ifelse(student=="Yes",1,0))
ggplot(data = df) +
geom_point(mapping = aes(x = balance, y = income, color = default)) +
theme(axis.text.x = element_text(size=25), axis.text.y = element_text(size=25),
axis.title=element_text(size=24,face="bold"))
# Model 2: logit(p) = b0 + b1*stdt (single 0/1 predictor variable)
Model2 <- glm(dft ~ stdt , data = df, family = "binomial")
Model4 <- glm(dft ~ balance + income + stdt, data = df, family = "binomial")
summary(Model4)
df <-  df %>%
mutate(pred_prob_model4 = predict(Model4, newdata = ., type = "response")) %>%
mutate(pred_outcome_model4 = ifelse(pred_prob_model4 >= 0.5,1,0))
predict(Model4, newdata = ., type = "response")
df$pred_prob_model4
predict(Model4, type = "response")
pred <- prediction(df$pred_prob_model4,df$dft)
auc.perf <-  performance(pred, measure = "auc")
auc.perf@y.values
prob = predict(bn, newdata = ., type = "response")
problabls = ifelse(prob >= 0.7, 1, 0)
pred = prediction(problabls, data$ï..admit)
auc = performance(pred, measure = 'auc')
auc@y.values
prob = predict(bn, newdata = ., type = "response")
problabls = ifelse(prob >= 0.3, 1, 0)
pred = prediction(problabls, data$ï..admit)
auc = performance(pred, measure = 'auc')
auc@y.values
prob = predict(bn, newdata = ., type = "response")
problabls = ifelse(prob >= 0.1, 1, 0)
pred = prediction(problabls, data$ï..admit)
auc = performance(pred, measure = 'auc')
auc@y.values
prob = predict(bn, newdata = ., type = "response")
problabls = ifelse(prob >= 0.2, 1, 0)
pred = prediction(problabls, data$ï..admit)
auc = performance(pred, measure = 'auc')
auc@y.values
prob = predict(bn, newdata = ., type = "response")
problabls = ifelse(prob >= 0.3, 1, 0)
pred = prediction(problabls, data$ï..admit)
auc = performance(pred, measure = 'auc')
auc@y.values
prob = predict(bn, newdata = ., type = "response")
problabls = ifelse(prob >= 0.4, 1, 0)
pred = prediction(problabls, data$ï..admit)
auc = performance(pred, measure = 'auc')
auc@y.values
prob = predict(bn, newdata = ., type = "response")
problabls = ifelse(prob >= 0.35, 1, 0)
pred = prediction(problabls, data$ï..admit)
auc = performance(pred, measure = 'auc')
auc@y.values
prob = predict(bn, newdata = ., type = "response")
problabls = ifelse(prob >= 0.25, 1, 0)
pred = prediction(problabls, data$ï..admit)
auc = performance(pred, measure = 'auc')
auc@y.values
prob = predict(bn, newdata = ., type = "response")
problabls = ifelse(prob >= 0.28, 1, 0)
pred = prediction(problabls, data$ï..admit)
auc = performance(pred, measure = 'auc')
auc@y.values
perf <- performance(pred, "tpr", "fpr") # tpr and fpr are true and false positive rates
plot(perf, colorize=T)
library(PerformanceAnalytics)
library(xts)
library(lubridate)
data = read.csv('Berkshire.csv')
table.Stats(data$BrkRet)
View(data)
myd(data$Date)
View(data)
data$Date
data$Date
as.Date.factor(data$Date)
as.Date(data$Date, "%m/%d/%Y")
data$Date = as.Date(data$Date, "%m/%d/%Y")
data$Date
d1 <- data %>%
filter(Date <= 2005-12-31)
View(data)
d1 <- data %>%
filter(Date <= '2005-12-31')
View(d1)
table.Stats(d1$BrkRet)
fund <- read.csv("contrafund.csv")
setwd("C:/Users/jonch/Desktop/Data Analytics Business/Week 6")
fund <- read.csv("contrafund.csv")
fund$Date <- mdy(fund$Date)
#Sorting data by dates
fund2<- fund[order(fund$Date),]
#create an xts dataset
All.dat <- xts(fund2[,-1],order.by = fund2[,1],)
#Calculate Compound Return for the fund across all the data
Return.cumulative(All.dat$ContraRet,geometric = TRUE)
#Cumulative Returns chart over time
#Check chart in Plots Tab on bottom right in R Studio
chart.CumReturns(All.dat$ContraRet,wealth.index = FALSE, geometric = TRUE)
############### Video 2: Measuring Risk
#Descriptive Statistics of the fund returns
#IMPORTANT NOTE - Arithmetic mean and standard deviation of returns are reported incorrectly
#The correct values as noticed when this code is run are:
#Arithmetic Mean Return = 1.17% and Standard Deviation = 4.32% per month
table.Stats(All.dat$ContraRet)
mean(d1$BrkRet-d1$MKT)
Return.cumulative(d1$BrkRet,geometric = TRUE)
Return.cumulative(data$BrkRet,geometric = TRUE)
(prod(d1$BrkRet+1)-1)*10000
Return.cumulative(d1$BrkRet,geometric = TRUE)*10000
SharpeRatio(d1$BrkRet,d1$RF)
d1[,-1]
xts(d1[,-1],order.by = d1[,1],)
ts=xts(d1[,-1],order.by = d1[,1],)
SharpeRatio(ts$BrkRet,ts$RF)
SharpeRatio(ts$MKT,ts$RF)
data = read.csv('Factor_HiTec.csv')
setwd("C:/Users/jonch/Desktop/Data Analytics Business/Exam 1")
data = read.csv('Factor_HiTec.csv')
data
lm1 = lm(data$HiTec_rf ~ data$Mkt_rf + data$SMB + data$HML + data$Mom + data$BAB + data$QMJ)
summary(lm1)
data = read.csv('UPS_KO.csv')
View(data)
d2 <- data %>%
filter(Date >=201504) %>%
filter(Date <=201811)
View(d2)
upsl= lm(d2$UPS ~ d2$Mkt_RF + d2$SMB + d2$HML)
kol= lm(d2$KO ~ d2$Mkt_RF + d2$SMB + d2$HML)
summary(upsl)
summary(kol)
library(stargazer)
install.packages('stargazer')
library(stargazer)
summary(upsl)
summary(kol)
data = College
#lin reg Personal with Room.board
lm1 = lm(formula = data$Personal ~ data$Room.Board)
summary(lm1) #.039, .038
#Question 1-5:
lm1 = lm(formula = data$Personal ~ data$Room.Board)
summary(lm1) #.03977, .03853
lnlog = lm(formula = data$Personal ~ log(data$Room.Board))
summary(lnlog) #.04037, .0391
loglog = lm(formula = log(data$Personal) ~ log(data$Room.Board))
summary(loglog) #.04489, .04366
logln = lm(formula =  log(data$Personal) ~ data$Room.Board)
summary(lnlog)
logln = lm(formula =  log(data$Personal) ~ data$Room.Board)
summary(logln)
mean(d1$BrkRet-d1$MKT)
